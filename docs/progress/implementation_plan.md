# Implementation Plan: Metrics & Telemetry Module

**Contexto**: Piloto de Valida√ß√£o (Dogfooding).
**Objetivo**: Implementar sistema de observabilidade para medir performance de agentes e skills.

## üß† Meta-Planning Analysis

### Desconstru√ß√£o do Pedido
*   **Goal**: "Medir performance".
*   **Implicit Needs**: Baixo overhead (n√£o deixar o sistema lento), armazenamento persistente mas simples, visualiza√ß√£o r√°pida via CLI.
*   **Constraints**: Bash-based (manter consist√™ncia com o core), depend√™ncia apenas de `jq`.

### Decis√µes Estrat√©gicas
1.  **Storage Agnostic**: Usar formato **JSONL (JSON Lines)** em `.aidev/state/metrics.log`.
    *   *Por que?* Append-only √© at√¥mico e r√°pido em filesystem. F√°cil de parsear com `jq` linha a linha sem carregar arquivo todo na mem√≥ria.
2.  **Instrumentation Points (Hooks)**:
    *   `lib/agents.sh`: Registrar start/finish de agentes.
    *   `lib/skills.sh`: Registrar uso de skills e sucesso/falha.
    *   `lib/mcp.sh`: (Futuro) Medir lat√™ncia de chamadas MCP.
3.  **Visualization**: Novo comando `aidev metrics` que agrega os dados on-the-fly usando `jq`.

## üèóÔ∏è Proposed Changes

### 1. Novo M√≥dulo Core: `lib/metrics.sh`
*   **Fun√ß√£o**: `metrics_track_event(type, name, duration, status, metadata)`
*   **Fun√ß√£o**: `metrics_start_timer(event_id)` -> retorna timestamp
*   **Fun√ß√£o**: `metrics_stop_timer(event_id)` -> calcula delta e grava

### 2. Instrumenta√ß√£o (Modify Existing Files)
#### [MODIFY] `lib/orchestration.sh`
*   Adicionar chamadas de m√©tricas nas fun√ß√µes `agent_activate`, `skill_run`.
*   Capturar falhas de recupera√ß√£o (`try_with_recovery`) como eventos de m√©trica.

### 3. Nova Interface CLI
#### [MODIFY] `bin/aidev`
*   Novo subcomando `metrics`.
*   Flags: `--summary` (padr√£o), `--agent <name>`, `--skill <name>`.

## üß™ Verification Plan (TDD First)

### Automated Tests (`tests/unit/test-metrics.sh`)
1.  **Test Storage**: Gravar um evento e verificar se o JSONL est√° v√°lido.
2.  **Test Performance**: Gravar 1000 eventos e medir impacto (deve ser < 50ms).
3.  **Test Aggregation**: Simular logs e verificar se o c√°lculo de "sucesso %" bate.

### Manual Verification
1.  Rodar `aidev start` (modo simula√ß√£o).
2.  Verificar se `.aidev/state/metrics.log` foi criado.
3.  Rodar `aidev metrics` e ver o dashboard.
